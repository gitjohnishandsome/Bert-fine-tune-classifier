from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import joblib
import pandas as pd
import os

# è¨­å®šè·¯å¾‘
base_dir = os.path.dirname(os.path.abspath(__file__))
model_dir = os.path.join(base_dir, "finetune_output_v2")  # å¾®èª¿å¾Œæ¨¡å‹è³‡æ–™å¤¾
test_path = os.path.join(base_dir, "test_data_500.csv")  # æ¸¬è©¦è³‡æ–™è·¯å¾‘
output_path = os.path.join(model_dir, "prediction_results.xlsx")  # é æ¸¬è¼¸å‡ºæª”æ¡ˆ

# è¼‰å…¥æ¨¡å‹èˆ‡ç·¨ç¢¼å™¨
tokenizer = AutoTokenizer.from_pretrained(model_dir)
model = AutoModelForSequenceClassification.from_pretrained(model_dir)
label_encoder = joblib.load(os.path.join(model_dir, "label_encoder.pkl"))

# è®€å–æ¸¬è©¦è³‡æ–™
df = pd.read_csv(test_path, encoding="utf-8")

# åˆå§‹åŒ–è¨ˆæ•¸å™¨èˆ‡çµæœåˆ—è¡¨
results = []
correct = 0
total = len(df)

# é æ¸¬æ¯ä¸€ç­†è³‡æ–™
for idx, row in df.iterrows():
    text = str(row["text"])
    label_true = row["label"]

    # æ–‡å­—è½‰å‘é‡
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        logits = model(**inputs).logits
        pred = torch.argmax(logits, dim=1).item()
        label_pred = label_encoder.inverse_transform([pred])[0]

    # æ˜¯å¦æ­£ç¢º
    is_correct = label_pred == label_true
    if is_correct:
        correct += 1

    # å­˜çµæœ
    results.append({
        "text": text,
        "true_label": label_true,
        "predicted_label": label_pred,
        "match": "âœ…" if is_correct else "âŒ"
    })

# æº–ç¢ºç‡
accuracy = correct / total
print(f"âœ… æ¸¬è©¦è³‡æ–™æº–ç¢ºç‡ï¼š{accuracy:.2%}")

# å­˜æˆ Excel
result_df = pd.DataFrame(results)
result_df.to_excel(output_path, index=False)  # âœ… åˆªé™¤ encoding åƒæ•¸
print(f"ğŸ“„ é æ¸¬çµæœå·²å„²å­˜è‡³ï¼š{output_path}")