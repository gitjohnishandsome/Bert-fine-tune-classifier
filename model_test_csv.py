from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import joblib
import os
import pandas as pd

# è¼‰å…¥ tokenizerã€modelã€label encoder
base_dir = os.path.dirname(os.path.abspath(__file__))
model_dir = os.path.join(base_dir, "finetune_output_v2")
test_path = os.path.join(base_dir, "testdata500_0605.csv")
ori_training_data_path = os.path.join(base_dir, "fine_tune_dataset_v4.csv")
last_training_data_path = os.path.join(base_dir, "fine_tune_dataset_v5.csv")

output_path = os.path.join(base_dir, "prediction_result_3.xlsx")

tokenizer = AutoTokenizer.from_pretrained(model_dir)
model = AutoModelForSequenceClassification.from_pretrained(model_dir)
label_encoder = joblib.load(os.path.join(model_dir, "label_encoder.pkl"))

# å˜—è©¦è®€å–æ¸¬è©¦è³‡æ–™
try:
    df = pd.read_csv(test_path, encoding="utf-8")
except UnicodeDecodeError:
    df = pd.read_csv(test_path, encoding="cp950")

df = df.dropna(subset=["text", "label"])  # ç§»é™¤ text æˆ– label æ˜¯ç©ºå€¼çš„åˆ—
df = df.reset_index(drop=True)  # é‡æ–°ç·¨è™Ÿï¼Œé¿å… index å‡ºéŒ¯
#df["text"] = df["text"].astype(str)
# å„²å­˜é æ¸¬èˆ‡æ¯”å°çµæœ
predicted_labels = []
match_results = []

total = len(df)
correct = 0

for idx, row in df.iterrows():
    inputs = tokenizer(row['text'], return_tensors="pt", truncation=True, padding=True)

    with torch.no_grad():
        logits = model(**inputs).logits
        pred = torch.argmax(logits, dim=1).item()
        predicted_label = label_encoder.inverse_transform([pred])[0]

    predicted_labels.append(predicted_label)

    if predicted_label == row["label"]:
        correct += 1
        match_results.append("âœ” æ­£ç¢º")
    else:
        match_results.append("âœ˜ éŒ¯èª¤")

# è¨ˆç®—æº–ç¢ºç‡
accuracy = correct / total
print(f"âœ… æ¸¬è©¦è³‡æ–™æº–ç¢ºç‡ï¼š{accuracy:.2%}")

# åŠ å…¥é æ¸¬æ¬„ä½
df["é æ¸¬çµæœ"] = predicted_labels
df["é…å°çµæœ"] = match_results

error_df = df[df["é…å°çµæœ"] == "âœ˜ éŒ¯èª¤"]
error_output_path = os.path.join(base_dir, "errors_for_finetune.csv")
error_df[["text", "label"]].to_csv(error_output_path, index=False, encoding="utf-8-sig")
# è¼¸å‡ºéŒ¯èª¤è³‡æ–™
print(f"âŒ éŒ¯èª¤è³‡æ–™å·²å„²å­˜è‡³ï¼š{error_output_path}")
# åˆä½µåŸå§‹è³‡æ–™èˆ‡éŒ¯èª¤è³‡æ–™

try:
    df_original = pd.read_csv(ori_training_data_path, encoding="utf-8")
    print("âœ… æˆåŠŸä»¥utf-8è®€å–df_original")
except UnicodeDecodeError:
    df_original = pd.read_csv(ori_training_data_path, encoding="cp950")
    print("âœ… æˆåŠŸä»¥cp950è®€å–df_original")

try:
    df_errors = pd.read_csv(error_output_path, encoding="utf-8")
    print("âœ… æˆåŠŸä»¥utf-8è®€å–df_errors")
except UnicodeDecodeError:
    df_errors = pd.read_csv(error_output_path, encoding="cp950")
    print("âœ… æˆåŠŸä»¥cp950è®€å–df_errors")

# ç·¨ç¢¼ä¸€è‡´è™•ç†ï¼ˆå»ºè­°ä¿ç•™ï¼‰
df_original["text"] = df_original["text"].astype(str)
df_errors["text"] = df_errors["text"].astype(str)

# åªä¿ç•™åœ¨éŒ¯èª¤è³‡æ–™ä¸­ã€Œä¸åœ¨åŸå§‹è³‡æ–™ä¸­çš„é …ç›®ã€
df_new = df_errors.merge(df_original, on=["text", "label"], how="left", indicator=True)
df_new = df_new[df_new["_merge"] == "left_only"].drop(columns=["_merge"])

# åˆä½µè³‡æ–™
df_combined = pd.concat([df_original, df_new], ignore_index=True)
df_combined.to_csv(last_training_data_path, index=False, encoding="utf-8-sig")
print(f"ğŸ“ å·²å°‡çµæœå„²å­˜è‡³ï¼š{last_training_data_path}")
# å„²å­˜ç‚º Excel
df.to_excel(output_path, index=False)
print(f"ğŸ“ å·²å°‡çµæœå„²å­˜è‡³ï¼š{output_path}")





